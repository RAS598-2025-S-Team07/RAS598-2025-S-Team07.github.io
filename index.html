
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://ras598-2025-s-team07.github.io/">
      
      
      
      
      <link rel="icon" href="static/logo.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.12">
    
    
      
        <title>Multipurpose Macro Load Bearing Assistive Follower Robot</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.2afb09e1.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="stylesheets/extra.css">
    
    <script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#multipurpose-macro-load-bearing-assistive-follower-robot-project" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="." title="Multipurpose Macro Load Bearing Assistive Follower Robot" class="md-header__button md-logo" aria-label="Multipurpose Macro Load Bearing Assistive Follower Robot" data-md-component="logo">
      
  <img src="static/logo.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Multipurpose Macro Load Bearing Assistive Follower Robot
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Multipurpose Macro Load Bearing Assistive Follower Robot Project
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/RAS598-2025-S-Team07/RAS598-2025-S-Team07.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="content.md" class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="." title="Multipurpose Macro Load Bearing Assistive Follower Robot" class="md-nav__button md-logo" aria-label="Multipurpose Macro Load Bearing Assistive Follower Robot" data-md-component="logo">
      
  <img src="static/logo.svg" alt="logo">

    </a>
    Multipurpose Macro Load Bearing Assistive Follower Robot
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/RAS598-2025-S-Team07/RAS598-2025-S-Team07.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="content.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#team-members" class="md-nav__link">
    <span class="md-ellipsis">
      Team Members
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Team Members">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rhutvik-pachghare" class="md-nav__link">
    <span class="md-ellipsis">
      Rhutvik Pachghare
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mohammad-nasr" class="md-nav__link">
    <span class="md-ellipsis">
      Mohammad Nasr
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shashank-sing-deo" class="md-nav__link">
    <span class="md-ellipsis">
      Shashank Sing Deo
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sensor-integration" class="md-nav__link">
    <span class="md-ellipsis">
      Sensor Integration
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#interaction" class="md-nav__link">
    <span class="md-ellipsis">
      Interaction
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Interaction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-collection-image-processing" class="md-nav__link">
    <span class="md-ellipsis">
      Data collection &amp; Image processing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shoe-detection-model-training-and-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Shoe Detection Model: Training and Inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dataset" class="md-nav__link">
    <span class="md-ellipsis">
      Dataset
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#control-and-autonomy" class="md-nav__link">
    <span class="md-ellipsis">
      Control and Autonomy
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Control and Autonomy">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#real-time-inference-with-depthai-and-oak-d-camera" class="md-nav__link">
    <span class="md-ellipsis">
      Real-Time Inference with DepthAI and OAK-D Camera
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-output" class="md-nav__link">
    <span class="md-ellipsis">
      Model Output
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-functionalities-of-predict_publisher_nodepy" class="md-nav__link">
    <span class="md-ellipsis">
      Key Functionalities of predict_publisher_node.py:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#navigation-and-motion-control" class="md-nav__link">
    <span class="md-ellipsis">
      Navigation and Motion Control
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#challenges-custom-development-and-solutions" class="md-nav__link">
    <span class="md-ellipsis">
      Challenges, Custom Development, and Solutions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#proposed-solutions-and-workarounds" class="md-nav__link">
    <span class="md-ellipsis">
      Proposed Solutions and Workarounds
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#finalized-ros-2-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Finalized ROS 2 Architecture
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#graphical-user-interface-gui" class="md-nav__link">
    <span class="md-ellipsis">
      Graphical User Interface (GUI)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Graphical User Interface (GUI)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#current-progress" class="md-nav__link">
    <span class="md-ellipsis">
      Current Progress:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#preparation-needs" class="md-nav__link">
    <span class="md-ellipsis">
      Preparation Needs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Preparation Needs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-do-you-need-to-know-to-be-successful" class="md-nav__link">
    <span class="md-ellipsis">
      What do you need to know to be successful?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#which-of-those-topics-do-you-need-to-cover-in-class" class="md-nav__link">
    <span class="md-ellipsis">
      Which of those topics do you need to cover in class?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#final-demonstration" class="md-nav__link">
    <span class="md-ellipsis">
      Final Demonstration
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#testing-evaluation-plan" class="md-nav__link">
    <span class="md-ellipsis">
      Testing &amp; Evaluation Plan
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Testing &amp; Evaluation Plan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#scenario-1-linear-and-turning-motion" class="md-nav__link">
    <span class="md-ellipsis">
      Scenario 1: Linear and Turning Motion
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scenario-2-random-path-tracking" class="md-nav__link">
    <span class="md-ellipsis">
      Scenario 2: Random Path Tracking
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scenario-3-obstacle-detection-and-alternate-path" class="md-nav__link">
    <span class="md-ellipsis">
      Scenario 3: Obstacle Detection and Alternate Path
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#impact" class="md-nav__link">
    <span class="md-ellipsis">
      Impact
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#project-advisor" class="md-nav__link">
    <span class="md-ellipsis">
      Project Advisor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#weekly-milestones" class="md-nav__link">
    <span class="md-ellipsis">
      Weekly Milestones
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="multipurpose-macro-load-bearing-assistive-follower-robot-project">Multipurpose Macro Load Bearing Assistive Follower Robot Project</h1>
<p><strong>Team:</strong> 07<br />
<strong>Course:</strong> RAS 598 Experimentation and Deployment of Robotic Systems, Arizona State University</p>
<h2 id="team-members">Team Members</h2>
<h3 id="rhutvik-pachghare">Rhutvik Pachghare</h3>
<p>Pursuing a master's degree in robotics and autonomous systems, with a degree in Electrical Engineering and experience as a team lead software developer, specializing in cloud computing and multiple development languages.</p>
<h3 id="mohammad-nasr">Mohammad Nasr</h3>
<p>Robotics and autonomous systems PhD student specializing in designing Bio-inspired hearing for robotic platforms.</p>
<h3 id="shashank-sing-deo">Shashank Sing Deo</h3>
<p>B.Tech in Mechanical Engineering, worked as systems and integration engineer with Evage motors designing N1 and M3 category vehicles for Indian, Middle east and European markets.</p>
<h2 id="introduction">Introduction</h2>
<p>This project investigates how a mobile robot can effectively follow and assist a user while maintaining situational awareness through obstacle avoidance and user recognition. Specifically, we focus on developing an assistive follower robot using a TurtleBot 4 platform equipped with a depth camera and LiDAR. The system is designed to enable hands-free transportation of small items—such as groceries, shopping bags, or tools—and assist in tasks like carrying a camera for photography or videography. Our broader objective is to contribute to the development and normalization of assistive robotic technologies in everyday life.</p>
<p>To operate effectively in real-world environments, the robot must support key features including global and local path planning, dynamic obstacle avoidance (e.g., navigating around moving pedestrians), and user-specific tracking to differentiate its intended user from others in the environment.</p>
<p>While the initial project vision included more ambitious capabilities—such as allowing the robot to lead the user, full SLAM integration, and advanced navigation using a gap-finding algorithm—hardware limitations (notably issues with the depth camera and the turtlebot4_navigation package) led us to revise the project's scope. The final implementation focuses on local path planning, obstacle avoidance, and user's shoe detection as a mean for user localization. These features, along with the rationale for our design decisions, are detailed in the following sections.</p>
<p><img alt="Project Idea" src="/assets/images/IMG_9859.PNG" title="functional depiction of the idea" /></p>
<h2 id="sensor-integration">Sensor Integration</h2>
<p>The assistive follower robot integrates both a depth camera and LiDAR to enable robust user tracking and situational awareness. The depth camera is primarily responsible for user recognition and tracking, while the LiDAR sensor is used for obstacle detection and environmental mapping. This combination allows the robot to navigate safely through dynamically changing environments.</p>
<p>Beyond obstacle avoidance, LiDAR can also be leveraged to maintain an optimal following distance. Once the user's direction relative to the robot is known—typically provided by the depth camera—the robot can associate this direction with a corresponding LiDAR beam and monitor its distance reading. Although this feature has not yet been implemented, it presents a promising opportunity to enhance the robot's control behavior and responsiveness in future iterations.</p>
<p><img alt="Sensor Integration" src="/assets/images/IMG_9871.PNG" title="Sensor Integration" /></p>
<h2 id="interaction">Interaction</h2>
<p>The TurtleBot 4 is equipped with a depth camera, LiDAR, and a Raspberry Pi, enabling effective sensing, interaction, and control. Due to the limited field of view of the depth camera and the computational constraints of the onboard Raspberry Pi, we adopted a lightweight user identification strategy based on shoe detection. A custom-trained model detects and localizes the user's shoes to estimate both the direction and distance of the user relative to the robot.</p>
<p>Our original design envisioned a hybrid navigation framework in which the LiDAR would provide 360-degree obstacle detection, assist in maintaining a safe following distance, and enable real-time gap-finding for navigation. In scenarios where no viable local path was identified through the gap-finding algorithm, the robot would fall back on a SLAM-based approach using the Nav2 stack to generate a local map and plan an alternative route. This layered architecture was intended to allow the robot to adapt intelligently to complex, dynamic environments.</p>
<p>However, due to persistent integration challenges with SLAM, Nav2, and the custom gap-finding algorithm, we revised our approach. The final implementation relies solely on local path planning, obstacle avoidance, and user direction input via shoe detection. In this setup, the robot receives the estimated direction of the user and initiates motion toward that direction using the gap-finding algorithm, while simultaneously maintaining environmental awareness and avoiding dynamic obstacles.</p>
<p><img alt="Interaction" src="/assets/images/IMG_9870.png" title="Interaction between various components" /></p>
<h3 id="data-collection-image-processing">Data collection &amp; Image processing</h3>
<p>To detect and localize shoes we focuse on developing a machine learning model capable of detecting shoes and estimating their position on-screen using an OAK-D camera mounted on a TurtleBot. The camera is fixed at a height of 192 mm from the ground, and its field of view has been calibrated to account for perspective distortion, including a measured offset of 180 mm between the camera’s optical center and the center of the captured image. </p>
<p><img alt="Data collection &amp; Image processing" src="/assets/images/Fig4.png" title="The captured image was analyzed to map the full viewing area and determine the region where the shoe is most accurately represented on the screen." /></p>
<p>Data collection was performed in a controlled indoor environment and classroom to ensure consistency, with the camera mounted on a tripod and a coordinate system marked on the floor to map image coordinates to physical positions as shown below. Data labeling was performed using the free and open-source software LabelMe, which allowed us to annotate each image with the shoe’s horizontal offset (x), distance from the camera (z), and orientation angle. The labeling convention we followed is illustrated in the figure below.</p>
<p><img alt="Data collection &amp; Image processing" src="/assets/images/Fig5.jpg" title="The current dataset covers only one lighting and environmental condition. For better generalization, future datasets will include varying lighting and scene conditions." /></p>
<p><img alt="Data collection &amp; Image processing" src="/assets/images/Fig5_1.png" /></p>
<p><img alt="Data collection &amp; Image processing" src="/assets/images/Fig6_1.png" /></p>
<p>To capture the variability in shoe placement, eight different images were taken at each floor coordinate. Additionally, to teach the model to infer depth, five images were captured at varying distances from a fixed origin for each shoe position. Each image in the dataset is annotated with the shoe's bounding box, its physical distance from the camera, and its relative floor coordinate.
As distance increases, both the bounding box size and screen mapping change due to perspective scaling. The proposed machine learning pipeline includes an object detection backbone—ResNet18—to identify the shoes and estimate the user’s direction and distance.</p>
<p><img alt="Data collection &amp; Image processing" src="/assets/images/Fig7.png" title="Camera screen layout showing shoe position at 1500mm" /></p>
<p><img alt="Data collection &amp; Image processing" src="/assets/images/Fig8.png" title="Distance cues such as shoe size in pixels and placement on the screen will help the model regress distance in addition to screen location." /></p>
<h3 id="shoe-detection-model-training-and-inference">Shoe Detection Model: Training and Inference</h3>
<p>To train the shoe detection and localization model, we first structured the dataset properly. The dataset directory includes a .csv annotation file, a folder containing all labeled images, and the Python training script. Each image in the dataset is labeled with normalized x-offset, z-distance from the camera, and orientation angle. A total of four values are regressed by the model.</p>
<p>We used a lightweight pretrained convolutional neural network—ResNet18—as the backbone for the regression task. Its final fully connected layer was modified to output four continuous values corresponding to the shoe’s location and orientation. The model was trained using PyTorch, with data augmentation (such as rotations) to improve generalization. The model was trained using the Mean Squared Error (MSE) loss function and optimized with Adam. After training, the model was saved in a .pth file for later use in real-time inference.</p>
<h3 id="dataset">Dataset</h3>
<p>Training data can be found <a href="https://drive.google.com/drive/folders/1v7vROEMcWfl_1NK7swpMpQlZ9D_5i1I-?usp=drive_link">here</a>.</p>
<h2 id="control-and-autonomy">Control and Autonomy</h2>
<p>This section outlines the core components of the robot’s autonomous behavior. We begin by describing the real-time inference method used to estimate the user's heading direction. Next, we detail the navigation and motion control strategies employed to guide the robot’s movement. Finally, we present the finalized ROS 2 architecture that integrates these components into a cohesive and functional system.</p>
<h3 id="real-time-inference-with-depthai-and-oak-d-camera">Real-Time Inference with DepthAI and OAK-D Camera</h3>
<p>The trained model is loaded and executed in real time using the OAK-D camera with DepthAI. The rgb_stereo launch file from the depthai_ros_driver package initializes the camera and publishes the RGB and stereo image streams to ROS 2 topics. These streams can be subscribed to for real-time image processing and pose estimation.
Our custom ROS 2 node, shoe_predictor_node.py, performs the complete inference pipeline and publishes the estimated user direction and position. This data is used by the TurtleBot to adjust its motion accordingly, enabling dynamic following behavior.</p>
<h3 id="model-output">Model Output</h3>
<p>The trained model outputs continuous values:</p>
<ul>
<li>
<p>Normalized x-offset (horizontal displacement of the shoe)</p>
</li>
<li>
<p>Normalized z-distance (depth from the camera)</p>
</li>
<li>
<p>Orientation angle (to estimate user heading)</p>
</li>
</ul>
<p>These values are processed to compute the user’s position relative to the robot and are published over ROS 2 for integration into the robot's control loop.</p>
<h3 id="key-functionalities-of-predict_publisher_nodepy">Key Functionalities of predict_publisher_node.py:</h3>
<ul>
<li>
<p>Image Subscription: Subscribes to the RGBD stream published by the OAK-D.</p>
</li>
<li>
<p>Model Inference: Loads the pre-trained ResNet18 model to detect and locate the user's shoes in the image.</p>
</li>
<li>
<p>Preprocessing: Converts incoming ROS image messages to OpenCV format using cv_bridge, resizes and normalizes the image to match the training format.</p>
</li>
<li>
<p>Prediction Processing: Uses model outputs to infer the user's relative direction, distance, and orientation.</p>
</li>
<li>
<p>Publishing: Sends estimated user relative direction to robot to the /rpi_11/user_position topic.</p>
</li>
<li>
<p>Error Handling: Logs issues during image conversion or inference to ensure robustness.</p>
</li>
</ul>
<p>This architecture enables frame-by-frame estimation of the user's shoe location and orientation in real time. Thanks to the lightweight nature of the ResNet18 model and efficient inference pipeline, this system is well-suited for deployment on resource-constrained platforms like the Raspberry Pi</p>
<h3 id="navigation-and-motion-control">Navigation and Motion Control</h3>
<p>Initially, we introduced the use of SLAM as a secondary navigation method. In this architecture, the robot primarily relies on a reactive gap-finding algorithm for real-time obstacle avoidance. However, in situations where a passable gap cannot be identified, the robot falls back on the SLAM-generated map to make informed navigation decisions. In this setup, a SLAM node runs continuously to map the environment as the robot moves. Once the user’s heading is determined, it is sent as a goal to the Nav2 planner server. This server generates a global path based on the current map. My custom Reactive Gap Finder node, which subscribes to the /rpi_11/plan topic, receives this path, follows it, and performs local obstacle avoidance by identifying and navigating through gaps in the environment.The figures below, which provide a zoomed-in view of the rqt_graph, clearly illustrate this architecture. As shown, the SLAM Toolbox subscribes to the /rpi_11/scan topic to perform mapping and localization. Meanwhile, the FollowerGap_node subscribes to the /rpi_11/plan topic to receive the global path coordinates for local planning.</p>
<p><img alt="Navigation and Motion Control" src="/assets/images/Fig9.png" title="Rqt_graph_SLAM" /></p>
<p><img alt="Navigation and Motion Control" src="/assets/images/Fig10.png" title="Rqt_graph_plan" /></p>
<p>From here, we present the key challenges encountered throughout the project, the custom solutions we implemented to address them, and the lessons learned during system integration and testing. This retrospective highlights both the technical hurdles we faced—such as SLAM, conflicting control inputs, and TF management—and the strategies we adopted to overcome them, including remapping, custom planner development, and refined system architecture.</p>
<h3 id="challenges-custom-development-and-solutions">Challenges, Custom Development, and Solutions</h3>
<ul>
<li><strong>SLAM Integration Issues:</strong>
One of our original objectives was to use SLAM-generated maps for enhanced path planning and localization. While we successfully demonstrated navigation using maps created in a controlled lab setting (see Figure and accompanying Navigation with Map video), this success was inconsistent and difficult to reproduce reliably. The core issues stemmed from turtlebot4_navigation package.</li>
</ul>
<p><img alt="Challenges, Custom Development, and Solutions" src="/assets/images/Fig11.png" title="Map" /></p>
<ul>
<li>
<p><strong>Obstacle Avoidance Conflicts:</strong>
The Reactive Gap Finder generally performed well in local navigation. However, occasional collisions with obstacles persisted, despite extensive parameter tuning. To mitigate this, we attempted to combine gap-based obstacle avoidance with Nav2’s dynamic mapping and planning features. Integration proved difficult due to command conflicts: both the Nav2 controller and our custom node were publishing velocity commands to the /cmd_vel topic simultaneously, resulting in erratic behavior and deviations from the intended path. Attempts to resolve this by modifying the nav2.yaml configuration file were unsuccessful, as demonstrated in our test video.</p>
</li>
<li>
<p><strong>Custom Development:</strong>
To overcome dependencies on the Nav2 stack, we explored a fully custom approach by developing a lightweight A* planner. This planner receives the estimated user position and direction as a goal and generates a corresponding trajectory. However, for this approach to be fully functional, proper access to the robot’s coordinate frames (TFs) is essential. Despite several attempts, we were unable to consistently obtain the necessary TFs (e.g., odom, base_link, map) within our custom node. The node was only able to access the map frame, and transformations between key frames were missing. As a result, although the robot could localize itself in the map frame, the lack of proper frame transformations caused it to deviate from the generated path as illustrated in our test video.</p>
</li>
</ul>
<h3 id="proposed-solutions-and-workarounds">Proposed Solutions and Workarounds</h3>
<ul>
<li><strong>Resolving "Full Queue" Errors and Dropped TF Messages</strong></li>
</ul>
<p>A common error we encountered involved message queues filling up and dropped TF messages. A practical solution was to remap TF topics explicitly:
/tf := /rpi_11/tf<br />
/tf_static := /rpi_11/tf_static</p>
<p>This remapping became necessary after I noticed an error message pop up when I closed RViz while SLAM and RViz were running together. The message highlighted improper remappings like /tf:=tf and /tf_static:=tf_static, which are ineffective.</p>
<ul>
<li><strong>Editing Configuration Files</strong></li>
</ul>
<p>In /opt/ros/humble/share/turtlebot4_navigation/config, YAML files define parameters for SLAM, localization, and the Nav2 stack. We experimented with editing these files to disable or replace certain components—such as removing the default planner server in favor of our own. However, unsuccessful, and likely due to additional internal dependencies that were not updated accordingly. This highlights the tight coupling between components in the Nav2 stack.</p>
<ul>
<li><strong>TFs Not Displaying with view_frames</strong></li>
</ul>
<p>Tools like view_frames from tf2_tools do not automatically account for namespaced TF topics. To visualize the full TF tree correctly, remapping is required at runtime:</p>
<p>ros2 run tf2_tools view_frames                             \
        --ros-args                                         \
        -r /tf:=/rpi_11/tf -r /tf_static:=/rpi_11/tf_static</p>
<p>After applying this, the TF tree was successfully generated and frames.pdf showed all expected transforms.</p>
<ul>
<li><strong>Using TFs in Custom Nodes</strong></li>
</ul>
<p>When using custom ROS 2 nodes that rely on TF data (e.g., for localization or planning), it is essential to launch them with the appropriate TF remappings. For example:</p>
<p>ros2 run my_nodes tiny_astar_planner \
  --ros-args \
  -r /tf:=/rpi_11/tf \
  -r /tf_static:=/rpi_11/tf_static</p>
<p>While this remapping allowed my node to receive TFs (such as the map frame), I encountered issues with incomplete transformations between frames. As Dr. Aukes recommended, a workaround is to record the TFs from RViz or explicitly publish them yourself (e.g., with a static transform publisher or a TF broadcaster node), ensuring your node has access to all required transforms.</p>
<h2 id="finalized-ros-2-architecture">Finalized ROS 2 Architecture</h2>
<p>Our final ROS 2 architecture consists of three primary nodes that work collaboratively to enable real-time user following and dynamic obstacle avoidance:</p>
<p><img alt="Finalized ROS 2 Architecture" src="/assets/images/output.png" title="Rqt_graph_plan" /></p>
<ul>
<li><strong>predictor_publisher_node</strong></li>
</ul>
<p>Subscribes to the RGB image stream from the OAK-D camera and uses a pretrained ResNet18 model to estimate the user’s horizontal offset, distance, and heading angle based on shoe detection. It publishes these predictions as formatted strings to /rpi_11/prediction_topic.</p>
<ul>
<li><strong>live_predict (via get_live_predictions())</strong></li>
</ul>
<p>Independently captures live frames from the OAK-D camera using DepthAI, processes them through the same model as camera_predictor_node, and yields raw prediction values. This node is used in non-ROS workflows and during development for debugging or visualization.</p>
<ul>
<li><strong>FollowGap_node (gap_finder)</strong></li>
</ul>
<p>Receives heading information from the prediction topic and LiDAR scan data. It performs scan preprocessing, detects navigable gaps, and selects the safest path that aligns with the user’s predicted direction. It then publishes velocity commands to /rpi_11/cmd_vel to drive the robot accordingly.</p>
<p>The camera_predictor_node and live_predict components together provide a robust estimate of the user's direction. This estimate is consumed by the gap_finder node, which ensures that navigation paths are not only safe but also aligned with the user's movements.</p>
<p>By fusing visual inference with LiDAR-based gap selection, the robot can adaptively and intelligently follow the user in dynamic environments, maintaining both safety and directional accuracy.</p>
<h2 id="graphical-user-interface-gui">Graphical User Interface (GUI)</h2>
<h3 id="current-progress">Current Progress:</h3>
<p>In this component of our project, we are simulating a ROS 2 NavigateToPose action server and integrate it with a PyQt5 graphical user interface (GUI) to visualize robot sensor data and navigation status. The action server acts as a dummy implementation that mimics the behavior of the standard NavigateToPose interface in ROS 2. It receives target pose goals from clients, simulates navigation by periodically publishing feedback (e.g., distance remaining), and returns a result upon completion to indicate whether the goal was successfully reached or not. Users can send goals using the ROS 2 command-line interface, specifying the desired position and orientation within the map frame. On the GUI side, we use PyQt5 to create a clean interface that displays LIDAR scan data, camera images, and the current status of the navigation process. The interface includes buttons to trigger visualizations and a status label that reflects the robot’s current state (e.g., Idle, Navigating, Completed). The GUI listens to the GoalStatusArray topic to receive real-time updates on the navigation goal, dynamically showing statuses such as Accepted, In Progress, or Aborted. In the final version, we replace the action server with the robot action server enabling us to monitor the navigation status. Loging buttons and camera will remain as tools for debugging purposes, while Lidar data display might not be needed in the final version. </p>
<p><img alt="Current Progress" src="/assets/images/Fig12.png" title="GUI" /></p>
<h2 id="preparation-needs">Preparation Needs</h2>
<h3 id="what-do-you-need-to-know-to-be-successful">What do you need to know to be successful?</h3>
<ul>
<li>Learn to calibrate and operate the hardware like camera, LiDAR, Raspberry Pi</li>
<li>Machine learning tools or OpenCV for optimal image processing</li>
<li>Data filtering</li>
<li>Control Logic and data flow</li>
</ul>
<h3 id="which-of-those-topics-do-you-need-to-cover-in-class">Which of those topics do you need to cover in class?</h3>
<ul>
<li>Data filtering</li>
<li>Control Logic and data flow</li>
</ul>
<h2 id="final-demonstration">Final Demonstration</h2>
<p>Conditions change in any environment. How will your robot handle variability in its environment?
- Potential environmental variabilities include lighting conditions, the user’s relative direction with respect to the robot, and narrow gaps smaller than the robot’s diameter.
- Changes in lighting conditions can affect image processing and user recognition accuracy. To address this, more advanced image processing algorithms that are robust to lighting variations will be explored.
- In scenarios where the robot encounters narrow gaps that are smaller than its diameter, the robot needs predefined behavior. In our case, The robot will use SLAM and mapping as an alternative mean of motion planning.
- To maintain accurate user tracking during movement, an optimal update rate for user position data will be determined. This will allow the robot to dynamically adjust the LiDAR beam it uses for distance keeping and movement direction correction, ensuring responsive and adaptive following behavior.</p>
<h2 id="testing-evaluation-plan">Testing &amp; Evaluation Plan</h2>
<h3 id="scenario-1-linear-and-turning-motion">Scenario 1: Linear and Turning Motion</h3>
<p>The robot follows the person in a straight line and takes turns as the person takes turns in a 2000mm x 1500mm test area divided into 500mm vertical sections.</p>
<p><img alt="Test Area 1" src="/assets/images/IMG_9868.PNG" title="Test Area 1" /></p>
<h3 id="scenario-2-random-path-tracking">Scenario 2: Random Path Tracking</h3>
<p>The robot must work autonomously and comprehend random paths traveled by the person. We plan to place start and stop markers for the target randomly in the test area with obstacles like traffic cones.</p>
<p><img alt="Test Area 2" src="/assets/images/IMG_9867.PNG" title="Test Area 2" /></p>
<h3 id="scenario-3-obstacle-detection-and-alternate-path">Scenario 3: Obstacle Detection and Alternate Path</h3>
<p>The robot may encounter obstacles in its path and must devise a new path to reach the target person.</p>
<p><img alt="Test Area 3" src="assets/images/IMG_9869.PNG" title="Test Area 3" /></p>
<h2 id="impact">Impact</h2>
<p>This project will advance assistive robotics by developing a follower robot that can safely navigate and help users in dynamic environments. It will help us learn advanced ROS2 communication, multi-sensor data fusion, and computer vision for real-time processing. We will also gain experience in dynamic path planning and obstacle avoidance. The knowledge gained will enhance our skills in robotics and autonomy. Our work can also contribute to course development by serving as an example helping future students.</p>
<h2 id="project-advisor">Project Advisor</h2>
<p><strong>Dr. Daniel Aukes</strong><br />
Dr. Daniel Aukes is an Associate Professor and the director of the IDEAlab. His research investigates the nexus of design, manufacturing, and data-driven decision-making to develop robots that can operate in niche environments, with a focus on affordability and accessibility. IDEALab projects focus on new tools for designing robots by pairing emerging fabrication techniques and new materials with analytical and simulation-based methods for understanding the behavior of complex systems. </p>
<h2 id="weekly-milestones">Weekly Milestones</h2>
<table>
<thead>
<tr>
<th>Week</th>
<th>Hardware Integration</th>
<th>Interface Development</th>
<th>Sensors</th>
<th>Controls &amp; Autonomy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Week 7</td>
<td>Set up TurtleBot 4, all team members access, configuration</td>
<td>N/A</td>
<td>Test data publishing from depth camera and LiDAR, IMU on ROS2 topics</td>
<td>N/A</td>
</tr>
<tr>
<td>Week 8</td>
<td>Calibrate depth camera for user recognition; Configure LiDAR</td>
<td>Learn Rviz and Gazebo</td>
<td>Verify sensor data accuracy and publish on ROS2 topics</td>
<td>N/A</td>
</tr>
<tr>
<td>Week 9</td>
<td>Ensure smooth data streaming to external PC for image processing</td>
<td>Use data visualization in real time with Rviz</td>
<td>Implement LiDAR, Depth Camera data</td>
<td>N/A</td>
</tr>
<tr>
<td>Week 10</td>
<td>Finalize hardware setup and connections</td>
<td>Implement interaction features: mode switching, emergency stop, and distance adjustment</td>
<td>Implement LiDAR, Depth Camera data</td>
<td>Try object avoidance</td>
</tr>
<tr>
<td>Week 11</td>
<td>N/A</td>
<td>User recognition modeling</td>
<td>Implement data acquisition and filtering for depth camera and LiDAR</td>
<td>Implement object avoidance</td>
</tr>
<tr>
<td>Week 12</td>
<td>N/A</td>
<td>User recognition modeling</td>
<td>Real time image streaming and prediction</td>
<td>N/A</td>
</tr>
<tr>
<td>Week 13</td>
<td>N/A</td>
<td>Real time user position update</td>
<td>Integrate sensor fusion</td>
<td>Implement motion control using user position feedback, and LiDAR data</td>
</tr>
<tr>
<td>Week 14</td>
<td>N/A</td>
<td>N/A</td>
<td>Final sensor tuning and data visualization</td>
<td>System integration testing and debugging</td>
</tr>
<tr>
<td>Week 15</td>
<td>Test Hardware in Demo Room</td>
<td>Test interaction features in Demo Room</td>
<td>Validate sensor accuracy and reliability</td>
<td>Final Test control logic and decision-making in dynamic environments</td>
</tr>
<tr>
<td>Week 16</td>
<td>Demo</td>
<td>Demo</td>
<td>Demo</td>
<td>Demo</td>
</tr>
</tbody>
</table>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2024 Team 7
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": ".", "features": ["navigation.tabs", "navigation.tabs.sticky", "toc.follow"], "search": "assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="javascripts/mathjax.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>