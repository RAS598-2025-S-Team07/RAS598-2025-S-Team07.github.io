
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://ras598-2025-s-team07.github.io/">
      
      
      
      
      <link rel="icon" href="static/logo.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.11">
    
    
      
        <title>Multipurpose Macro Load Bearing Assistive Follower Robot</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.4af4bdda.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="stylesheets/extra.css">
    
    <script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#welcome-to-the-multipurpose-macro-load-bearing-assistive-follower-robot-project" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="." title="Multipurpose Macro Load Bearing Assistive Follower Robot" class="md-header__button md-logo" aria-label="Multipurpose Macro Load Bearing Assistive Follower Robot" data-md-component="logo">
      
  <img src="static/logo.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Multipurpose Macro Load Bearing Assistive Follower Robot
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Welcome to the Multipurpose Macro Load Bearing Assistive Follower Robot Project
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/RAS598-2025-S-Team07/RAS598-2025-S-Team07.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="content.md" class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="." title="Multipurpose Macro Load Bearing Assistive Follower Robot" class="md-nav__button md-logo" aria-label="Multipurpose Macro Load Bearing Assistive Follower Robot" data-md-component="logo">
      
  <img src="static/logo.svg" alt="logo">

    </a>
    Multipurpose Macro Load Bearing Assistive Follower Robot
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/RAS598-2025-S-Team07/RAS598-2025-S-Team07.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="content.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#team-members" class="md-nav__link">
    <span class="md-ellipsis">
      Team Members
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Team Members">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rhutvik-pachghare" class="md-nav__link">
    <span class="md-ellipsis">
      Rhutvik Pachghare
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mohammad-nasr" class="md-nav__link">
    <span class="md-ellipsis">
      Mohammad Nasr
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shashank-sing-deo" class="md-nav__link">
    <span class="md-ellipsis">
      Shashank Sing Deo
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sensor-integration" class="md-nav__link">
    <span class="md-ellipsis">
      Sensor Integration
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#interaction" class="md-nav__link">
    <span class="md-ellipsis">
      Interaction
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Interaction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-collection-image-processing" class="md-nav__link">
    <span class="md-ellipsis">
      Data collection &amp; Image processing
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#control-and-autonomy" class="md-nav__link">
    <span class="md-ellipsis">
      Control and Autonomy
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Control and Autonomy">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#current-progress" class="md-nav__link">
    <span class="md-ellipsis">
      Current Progress
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#graphical-user-interface-gui" class="md-nav__link">
    <span class="md-ellipsis">
      Graphical User Interface (GUI)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Graphical User Interface (GUI)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#current-progress_1" class="md-nav__link">
    <span class="md-ellipsis">
      Current Progress:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#preparation-needs" class="md-nav__link">
    <span class="md-ellipsis">
      Preparation Needs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Preparation Needs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-do-you-need-to-know-to-be-successful" class="md-nav__link">
    <span class="md-ellipsis">
      What do you need to know to be successful?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#which-of-those-topics-do-you-need-to-cover-in-class" class="md-nav__link">
    <span class="md-ellipsis">
      Which of those topics do you need to cover in class?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#final-demonstration" class="md-nav__link">
    <span class="md-ellipsis">
      Final Demonstration
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#testing-evaluation-plan" class="md-nav__link">
    <span class="md-ellipsis">
      Testing &amp; Evaluation Plan
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Testing &amp; Evaluation Plan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#scenario-1-linear-and-turning-motion" class="md-nav__link">
    <span class="md-ellipsis">
      Scenario 1: Linear and Turning Motion
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scenario-2-random-path-tracking" class="md-nav__link">
    <span class="md-ellipsis">
      Scenario 2: Random Path Tracking
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scenario-3-obstacle-detection-and-alternate-path" class="md-nav__link">
    <span class="md-ellipsis">
      Scenario 3: Obstacle Detection and Alternate Path
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#impact" class="md-nav__link">
    <span class="md-ellipsis">
      Impact
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#project-advisor" class="md-nav__link">
    <span class="md-ellipsis">
      Project Advisor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#weekly-milestones" class="md-nav__link">
    <span class="md-ellipsis">
      Weekly Milestones
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="welcome-to-the-multipurpose-macro-load-bearing-assistive-follower-robot-project">Welcome to the Multipurpose Macro Load Bearing Assistive Follower Robot Project</h1>
<p><strong>Team:</strong> 07<br />
<strong>Course:</strong> RAS 598 Experimentation and Deployment of Robotic Systems, Arizona State University</p>
<h2 id="team-members">Team Members</h2>
<h3 id="rhutvik-pachghare">Rhutvik Pachghare</h3>
<p>Pursuing a master's degree in robotics and autonomous systems, with a degree in Electrical Engineering and experience as a team lead software developer, specializing in cloud computing and multiple development languages.</p>
<h3 id="mohammad-nasr">Mohammad Nasr</h3>
<p>Robotics and autonomous systems PhD student specializing in designing Bio-inspired hearing for robotic platforms.</p>
<h3 id="shashank-sing-deo">Shashank Sing Deo</h3>
<p>B.Tech in Mechanical Engineering, worked as systems and integration engineer with Evage motors designing N1 and M3 category vehicles for Indian, Middle east and European markets.</p>
<h2 id="introduction">Introduction</h2>
<p>This project seeks to explore how a mobile robotic system can effectively follow and assist a user while maintaining situational awareness through obstacle detection and user recognition. Specifically, we aim to develop a follower robot using a TurtleBot equipped with a depth camera and Lidar. This system will enable hands-free transportation of small items (e.g., groceries, shopping bags, or tools) and assist with tasks like carrying a camera for photography or videography. Through this research, we hope to contribute to the advancement and normalization of Assistive Robots Technology in everyday life.</p>
<p><img alt="Project Idea" src="/assets/images/IMG_9859.PNG" title="functional depiction of the idea" /></p>
<h2 id="sensor-integration">Sensor Integration</h2>
<p>The assistive follower robot will utilize a combination of the depth camera and LiDAR to achieve robust user tracking and situational awareness. The depth camera will be used for user recognition and tracking, while LiDAR will be used to ensure the robot maintains an optimal following distance. Additionally, LiDAR can be used for 360-degree obstacle detection and environmental mapping, allowing the robot to navigate dynamically changing environments safely.</p>
<p><img alt="Sensor Integration" src="/assets/images/IMG_9871.PNG" title="Sensor Integration" /></p>
<h2 id="interaction">Interaction</h2>
<p>The TurtleBot 4 is equipped with a depth camera, LiDAR, and a Raspberry Pi board, enabling effective interaction and control. Due to the limited field of view of the depth camera and the desire to reduce computational load on the Raspberry Pi, we have adopted shoe detection as a lightweight and focused approach for user identification. A custom-trained model will detect the user's shoes, allowing us to estimate their direction relative to the image center. The image data will be processed externally on a PC or online on the raspberry pi, and the results will be used to guide robot's motion.</p>
<p>LiDAR will be used for 360-degree obstacle detection and distance maintenance, and will play a central role in real-time gap-finding navigation. In situations where the robot is unable to find a viable local path using the gap-finding algorithm, it will fall back on a pre-built SLAM map to determine an alternative route. This layered approach allows the robot to adapt intelligently to dynamic and complex environments.</p>
<p><img alt="Interaction" src="/assets/images/IMG_9870.png" title="Interaction between various components" /></p>
<h3 id="data-collection-image-processing">Data collection &amp; Image processing</h3>
<p>To detect and localize shoes we focuse on developing a machine learning model capable of detecting shoes and estimating their position on-screen using an OAK-D camera mounted on a TurtleBot. The camera is fixed at a height of 192 mm from the ground, and its field of view has been calibrated to account for perspective distortion, including a measured offset of 180 mm between the camera’s optical center and the center of the captured image. Data collection was performed in a controlled indoor environment to ensure consistency, with the camera mounted on a tripod and a coordinate system marked on the floor to map image coordinates to physical positions. To capture the variability in shoe placement, eight different images were taken at each floor coordinate. Additionally, to teach the model to infer depth, five images were captured at varying distances from a fixed origin for each shoe position. Each image in the dataset is annotated with the shoe's bounding box, its physical distance from the camera, and its relative floor coordinate. As distance increases, both the bounding box size and screen mapping change due to perspective scaling. The proposed machine learning pipeline includes an object detection backbone—such as YOLOv8 or SSD—to identify shoes and predict bounding boxes, followed by a regression-based localization head to estimate the shoe's screen coordinates and, optionally, its physical distance from the camera. The overall methodology consists of several steps: data collection using the calibrated camera setup, preprocessing involving image normalization and augmentation (e.g., rotation and brightness variation), supervised training using annotated labels, rigorous evaluation on unseen environments and lighting conditions, and finally, real-time deployment integrated with the OAK-D camera’s live stream to enable real-time shoe localization on the robot.
<img alt="Data collection &amp; Image processing" src="/assets/images/Fig4.png" title="The captured image was analyzed to map the full viewing area and determine the region where the shoe is most accurately represented on the screen." /></p>
<h2 id="control-and-autonomy">Control and Autonomy</h2>
<p>The depth camera will be used to detect the user's shoes in real-time, providing a simple yet effective method for user recognition and direction estimation. The image stream will be processed externally on our laptop or online on the raspberry pi, where a custom-trained model will identify the location of the shoes in the frame. This information will be translated into the user’s direction relative to the robot and communicated back to the TurtleBot over ROS2, allowing it to adjust its motion accordingly.</p>
<p>LiDAR will be the primary sensor for dynamic obstacle detection, gap-finding, and distance maintenance. It will also help refine the robot’s understanding of the user’s position by correlating the visual direction from the camera with the LiDAR scan to estimate the user’s distance. In cases where the gap-finding algorithm cannot identify a safe path forward, the robot will utilize a prebuilt SLAM map to determine an alternative route around obstacles, ensuring reliable and adaptive navigation in cluttered environments.</p>
<h3 id="current-progress">Current Progress</h3>
<p>In our previous revision, we introduced the use of SLAM as a secondary navigation method. In this architecture, the robot primarily relies on a reactive gap-finding algorithm for real-time obstacle avoidance. However, in situations where a passable gap cannot be identified, the robot falls back on the SLAM-generated map to make informed navigation decisions.
While the core logic of our system remains unchanged, we have made several refinements. In the updated setup, a SLAM node runs continuously to map the environment as the robot moves. Once the user’s heading is determined, it is sent as a goal to the Nav2 planner server. This server generates a global path based on the current map. My custom Reactive Gap Finder node, which subscribes to the /rpi_11/plan topic, receives this path, follows it, and performs local obstacle avoidance by identifying and navigating through gaps in the environment.</p>
<p><strong>Current Challenges:</strong></p>
<ul>
<li>
<p><strong>SLAM Integration Issues:</strong> Although our goal was to use the SLAM-generated map for secondary path planning, issues with the <code>turtlebot4_navigation</code> package have made this integration difficult. I was able to successfully demonstrate the robot navigating using a map generated from my lab environment (see Figure 1 and the Navigation with Map video), but this success was not consistently reproducible.</p>
</li>
<li>
<p><strong>Obstacle Avoidance Conflicts:</strong> While the Reactive Gap Finder generally performs well, I observed that the robot occasionally collides with obstacles despite significant parameter tuning. This is the primary motivation behind fusing it with the Nav2 dynamic mapping and planning capabilities. However, integration poses a challenge: to avoid conflicting commands, we must isolate the <code>planner_server</code> from the full <code>turtlebot4_navigation</code> stack. When both the Nav2 controller and my reactive gap logic publish to the <code>/cmd_vel</code> topic, the robot receives mixed commands and deviates from the intended path. This behavior is shown in the accompanying demonstration video. Attempts to resolve this by editing the <code>nav2.yaml</code> file were unsuccessful.</p>
</li>
</ul>
<p><strong>Potential Final Solution:</strong>
In case we can not use the nav2 and its planner server, we stick to my Reactive gap finder and we make sure to use the gaps that are in the direction of user. </p>
<h2 id="graphical-user-interface-gui">Graphical User Interface (GUI)</h2>
<h3 id="current-progress_1">Current Progress:</h3>
<p>In this component of our project, we are simulate a ROS 2 NavigateToPose action server and integrate it with a PyQt5 graphical user interface (GUI) to visualize robot sensor data and navigation status. The action server acts as a dummy implementation that mimics the behavior of the standard NavigateToPose interface in ROS 2. It receives target pose goals from clients, simulates navigation by periodically publishing feedback (e.g., distance remaining), and returns a result upon completion to indicate whether the goal was successfully reached or not. Users can send goals using the ROS 2 command-line interface, specifying the desired position and orientation within the map frame. On the GUI side, we use PyQt5 to create a clean interface that displays LIDAR scan data, camera images, and the current status of the navigation process. The interface includes buttons to trigger visualizations and a status label that reflects the robot’s current state (e.g., Idle, Navigating, Completed). The GUI listens to the GoalStatusArray topic to receive real-time updates on the navigation goal, dynamically showing statuses such as Accepted, In Progress, or Aborted. In the final version, we replace the action server with the robot action server enabling us to monitor the navigation status. Loging buttons and camera will remain as tools for debugging purposes, while Lidar data display might not be needed in the final version. </p>
<h2 id="preparation-needs">Preparation Needs</h2>
<h3 id="what-do-you-need-to-know-to-be-successful">What do you need to know to be successful?</h3>
<ul>
<li>Learn to calibrate and operate the hardware like camera, LiDAR, Raspberry Pi</li>
<li>Machine learning tools or OpenCV for optimal image processing</li>
<li>Data filtering</li>
<li>Control Logic and data flow</li>
</ul>
<h3 id="which-of-those-topics-do-you-need-to-cover-in-class">Which of those topics do you need to cover in class?</h3>
<ul>
<li>Data filtering</li>
<li>Control Logic and data flow</li>
</ul>
<h2 id="final-demonstration">Final Demonstration</h2>
<p>Conditions change in any environment. How will your robot handle variability in its environment?
- Potential environmental variabilities include lighting conditions, the user’s relative direction with respect to the robot, and narrow gaps smaller than the robot’s diameter.
- Changes in lighting conditions can affect image processing and user recognition accuracy. To address this, more advanced image processing algorithms that are robust to lighting variations will be explored.
- In scenarios where the robot encounters narrow gaps that are smaller than its diameter, the robot needs predefined behavior. In our case, The robot will use SLAM and mapping as an alternative mean of motion planning.
- To maintain accurate user tracking during movement, an optimal update rate for user position data will be determined. This will allow the robot to dynamically adjust the LiDAR beam it uses for distance keeping and movement direction correction, ensuring responsive and adaptive following behavior.</p>
<h2 id="testing-evaluation-plan">Testing &amp; Evaluation Plan</h2>
<h3 id="scenario-1-linear-and-turning-motion">Scenario 1: Linear and Turning Motion</h3>
<p>The robot follows the person in a straight line and takes turns as the person takes turns in a 2000mm x 1500mm test area divided into 500mm vertical sections.</p>
<p><img alt="Test Area 1" src="/assets/images/IMG_9868.PNG" title="Test Area 1" /></p>
<h3 id="scenario-2-random-path-tracking">Scenario 2: Random Path Tracking</h3>
<p>The robot must work autonomously and comprehend random paths traveled by the person. We plan to place start and stop markers for the target randomly in the test area with obstacles like traffic cones.</p>
<p><img alt="Test Area 2" src="/assets/images/IMG_9867.PNG" title="Test Area 2" /></p>
<h3 id="scenario-3-obstacle-detection-and-alternate-path">Scenario 3: Obstacle Detection and Alternate Path</h3>
<p>The robot may encounter obstacles in its path and must devise a new path to reach the target person.</p>
<p><img alt="Test Area 3" src="assets/images/IMG_9869.PNG" title="Test Area 3" /></p>
<h2 id="impact">Impact</h2>
<p>This project will advance assistive robotics by developing a follower robot that can safely navigate and help users in dynamic environments. It will help us learn advanced ROS2 communication, multi-sensor data fusion, and computer vision for real-time processing. We will also gain experience in dynamic path planning and obstacle avoidance. The knowledge gained will enhance our skills in robotics and autonomy. Our work can also contribute to course development by serving as an example helping future students.</p>
<h2 id="project-advisor">Project Advisor</h2>
<p><strong>Dr. Daniel Aukes</strong><br />
Dr. Daniel Aukes is an Associate Professor and the director of the IDEAlab. His research investigates the nexus of design, manufacturing, and data-driven decision-making to develop robots that can operate in niche environments, with a focus on affordability and accessibility. IDEALab projects focus on new tools for designing robots by pairing emerging fabrication techniques and new materials with analytical and simulation-based methods for understanding the behavior of complex systems. </p>
<h2 id="weekly-milestones">Weekly Milestones</h2>
<table>
<thead>
<tr>
<th>Week</th>
<th>Hardware Integration</th>
<th>Interface Development</th>
<th>Sensors</th>
<th>Controls &amp; Autonomy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Week 7</td>
<td>Set up TurtleBot 4, all team members access, configuration</td>
<td>N/A</td>
<td>Test data publishing from depth camera and LiDAR, IMU on ROS2 topics</td>
<td>N/A</td>
</tr>
<tr>
<td>Week 8</td>
<td>Calibrate depth camera for user recognition; Configure LiDAR</td>
<td>Learn Rviz and Gazebo</td>
<td>Verify sensor data accuracy and publish on ROS2 topics</td>
<td>N/A</td>
</tr>
<tr>
<td>Week 9</td>
<td>Ensure smooth data streaming to external PC for image processing</td>
<td>Use data visualization in real time with Rviz</td>
<td>Implement LiDAR, Depth Camera data</td>
<td>N/A</td>
</tr>
<tr>
<td>Week 10</td>
<td>Finalize hardware setup and connections</td>
<td>Implement interaction features: mode switching, emergency stop, and distance adjustment</td>
<td>Implement LiDAR, Depth Camera data</td>
<td>Try object avoidance</td>
</tr>
<tr>
<td>Week 11</td>
<td>N/A</td>
<td>User recognition modeling</td>
<td>Implement data acquisition and filtering for depth camera and LiDAR</td>
<td>Implement object avoidance</td>
</tr>
<tr>
<td>Week 12</td>
<td>N/A</td>
<td>User recognition modeling</td>
<td>Real time image streaming and prediction</td>
<td>N/A</td>
</tr>
<tr>
<td>Week 13</td>
<td>N/A</td>
<td>Real time user position update</td>
<td>Integrate sensor fusion</td>
<td>Implement motion control using user position feedback, and LiDAR data</td>
</tr>
<tr>
<td>Week 14</td>
<td>N/A</td>
<td>N/A</td>
<td>Final sensor tuning and data visualization</td>
<td>System integration testing and debugging</td>
</tr>
<tr>
<td>Week 15</td>
<td>Test Hardware in Demo Room</td>
<td>Test interaction features in Demo Room</td>
<td>Validate sensor accuracy and reliability</td>
<td>Final Test control logic and decision-making in dynamic environments</td>
</tr>
<tr>
<td>Week 16</td>
<td>Demo</td>
<td>Demo</td>
<td>Demo</td>
<td>Demo</td>
</tr>
</tbody>
</table>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2024 Team 7
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": ".", "features": ["navigation.tabs", "navigation.tabs.sticky", "toc.follow"], "search": "assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="javascripts/mathjax.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>