{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"content/","title":"Welcome to the Multipurpose Macro Load Bearing Assistive Follower Robot Project","text":"<p>Team: 07 Course: RAS 598 Experimentation and Deployment of Robotic Systems, Arizona State University</p>"},{"location":"content/#team-members","title":"Team Members","text":""},{"location":"content/#rhutvik-pachghare","title":"Rhutvik Pachghare","text":"<p>Pursuing a master's degree in robotics and autonomous systems, with a degree in Electrical Engineering and experience as a team lead software developer, specializing in cloud computing and multiple development languages.</p>"},{"location":"content/#mohammad-nasr","title":"Mohammad Nasr","text":"<p>Robotics and autonomous systems PhD student specializing in designing Bio-inspired hearing for robotic platforms.</p>"},{"location":"content/#shashank-sing-deo","title":"Shashank Sing Deo","text":"<p>B.Tech in Mechanical Engineering, worked as systems and integration engineer with Evage motors designing N1 and M3 category vehicles for Indian, Middle east and European markets.</p>"},{"location":"content/#introduction","title":"Introduction","text":"<p>This project seeks to explore how a mobile robotic system can effectively follow and assist a user while maintaining situational awareness through obstacle detection and user recognition. Specifically, we aim to develop a follower robot using a TurtleBot equipped with a depth camera and Lidar. This system will enable hands-free transportation of small items (e.g., groceries, shopping bags, or tools) and assist with tasks like carrying a camera for photography or videography. Additionally, we will investigate how the robot can not only follow but also lead the user while maintaining a virtual leash, enhancing user experience and efficiency in personal and professional applications. Through this research, we hope to contribute to the advancement and normalization of Assistive Robots Technology in everyday life.</p> <p></p>"},{"location":"content/#sensor-integration","title":"Sensor Integration","text":"<p>The assistive follower robot will utilize a combination of the depth camera and LiDAR to achieve robust user tracking and situational awareness. The depth camera will be used for user recognition and tracking, while LiDAR will be used to ensure the robot maintains an optimal following distance. Additionally, LiDAR can be used for 360-degree obstacle detection and environmental mapping, allowing the robot to navigate dynamically changing environments safely.</p> <p></p>"},{"location":"content/#interaction","title":"Interaction","text":"<p>The TurtleBot 4 is equipped with a depth camera, LiDAR, and a Raspberry Pi board, enabling advanced interaction and control capabilities. The depth camera can stream images over a ROS2 topic, allowing high-level image processing tasks such as walking pattern recognition or face recognition. To ensure smooth and real-time processing, the image data will be streamed to our PC with more computational power. The results, such as user position or identity, will then be communicated back to the TurtleBot over ROS2, influencing its movement decisions accordingly. LiDAR will provide 360-degree obstacle detection, environmental mapping, and can be used to ensure the robot maintains an optimal following distance while navigating the environment. Moreover, by integrating LiDAR data with a Gap-finding algorithm, the robot will intelligently identify safe passages between obstacles, enabling collision-free navigation even in complex surroundings.</p> <p></p>"},{"location":"content/#control-and-autonomy","title":"Control and Autonomy","text":"<p>The depth camera will provide real-time user tracking and recognition data, which will be processed on an external PC for advanced image processing tasks. The processed information, such as the user\u2019s position or direction relative to the robot, will then be communicated back to the TurtleBot as feedback, enabling it to adjust its direction of motion accordingly. LiDAR feedback will be primarily used for dynamic obstacle detection and maintaining an optimal distance from the user and surrounding walls. Additionally, LiDAR data will enhance the accuracy of depth information by cross-referencing user position. Specifically, the user\u2019s relative angle obtained from image data will inform the robot about which LiDAR beam is detecting the user, thereby providing precise positional information. Finally, by integrating LiDAR data with the Gap-finding algorithm, the robot will intelligently identify safe passages between obstacles and navigate through them without collisions.</p>"},{"location":"content/#preparation-needs","title":"Preparation Needs","text":""},{"location":"content/#what-do-you-need-to-know-to-be-successful","title":"What do you need to know to be successful?","text":"<ul> <li>To use a Nvidia Jetson Nano with a camera and LiDAR</li> <li>Learn to calibrate and operate the hardware like camera, LiDAR, Raspberry Pi</li> <li>Machine learning tools or OpenCV for optimal image processing</li> <li>Data filtering</li> <li>Control Logic and data flow</li> </ul>"},{"location":"content/#which-of-those-topics-do-you-need-to-cover-in-class","title":"Which of those topics do you need to cover in class?","text":"<ul> <li>To use a Nvidia Jetson Nano with a camera and LiDAR</li> <li>Data filtering</li> <li>Control Logic and data flow</li> </ul>"},{"location":"content/#final-demonstration","title":"Final Demonstration","text":"<p>Conditions change in any environment. How will your robot handle variability in its environment? - Potential environmental variabilities include lighting conditions, the user\u2019s relative direction with respect to the robot, and narrow gaps smaller than the robot\u2019s diameter. - Changes in lighting conditions can affect image processing and user recognition accuracy. To address this, more advanced image processing algorithms that are robust to lighting variations will be explored. - In scenarios where the robot encounters narrow gaps that are smaller than its diameter, the robot needs predefined behavior. For instance, the robot can either stop or emit a sound to alert the user, ensuring safe navigation without collisions. - To maintain accurate user tracking during movement, an optimal update rate for user position data will be determined. This will allow the robot to dynamically adjust the LiDAR beam it uses for distance keeping and movement direction correction, ensuring responsive and adaptive following behavior.</p>"},{"location":"content/#testing-evaluation-plan","title":"Testing &amp; Evaluation Plan","text":""},{"location":"content/#scenario-1-linear-and-turning-motion","title":"Scenario 1: Linear and Turning Motion","text":"<p>The robot follows the person in a straight line and takes turns as the person takes turns in a 2000mm x 1500mm test area divided into 500mm vertical sections.</p> <p></p>"},{"location":"content/#scenario-2-random-path-tracking","title":"Scenario 2: Random Path Tracking","text":"<p>The robot must work autonomously and comprehend random paths traveled by the person. We plan to place start and stop markers for the target randomly in the test area with obstacles like traffic cones.</p> <p></p>"},{"location":"content/#scenario-3-obstacle-detection-and-alternate-path","title":"Scenario 3: Obstacle Detection and Alternate Path","text":"<p>The robot may encounter obstacles in its path and must devise a new path to reach the target person.</p> <p></p>"},{"location":"content/#impact","title":"Impact","text":"<p>This project will advance assistive robotics by developing a follower robot that can safely navigate and help users in dynamic environments. It will help us learn advanced ROS2 communication, multi-sensor data fusion, and computer vision for real-time processing. We will also gain experience in dynamic path planning and obstacle avoidance. The knowledge gained will enhance our skills in robotics and autonomy. Our work can also contribute to course development by serving as an example helping future students.</p>"},{"location":"content/#project-advisor","title":"Project Advisor","text":"<p>Dr. Daniel Aukes Dr. Daniel Aukes is an Associate Professor and the director of the IDEAlab. His research investigates the nexus of design, manufacturing, and data-driven decision-making to develop robots that can operate in niche environments, with a focus on affordability and accessibility. IDEALab projects focus on new tools for designing robots by pairing emerging fabrication techniques and new materials with analytical and simulation-based methods for understanding the behavior of complex systems.</p>"},{"location":"content/#weekly-milestones","title":"Weekly Milestones","text":"Week Hardware Integration Interface Development Sensors Controls &amp; Autonomy Week 7 Set up TurtleBot 4, all team members access, configuration N/A Test data publishing from depth camera and LiDAR, IMU on ROS2 topics N/A Week 8 Calibrate depth camera for user recognition; Configure LiDAR Learn Rviz and Gazebo Verify sensor data accuracy and publish on ROS2 topics N/A Week 9 Ensure smooth data streaming to external PC for image processing Use data visualization in real time with Rviz Implement LiDAR, Depth Camera data N/A Week 10 Finalize hardware setup and connections Implement interaction features: mode switching, emergency stop, and distance adjustment Implement LiDAR, Depth Camera data Try object avoidance Week 11 N/A User recognition modeling Implement data acquisition and filtering for depth camera and LiDAR Implement object avoidance Week 12 N/A User recognition modeling Real time image streaming and prediction N/A Week 13 N/A Real time user position update Integrate sensor fusion Implement motion control using user position feedback, and LiDAR data Week 14 N/A N/A Final sensor tuning and data visualization System integration testing and debugging Week 15 Test Hardware in Demo Room Test interaction features in Demo Room Validate sensor accuracy and reliability Final Test control logic and decision-making in dynamic environments Week 16 Demo Demo Demo Demo"},{"location":"static/node_modules/mathjax/","title":"MathJax","text":""},{"location":"static/node_modules/mathjax/#beautiful-math-in-all-browsers","title":"Beautiful math in all browsers","text":"<p>MathJax is an open-source JavaScript display engine for LaTeX, MathML, and AsciiMath notation that works in all modern browsers.  It was designed with the goal of consolidating the recent advances in web technologies into a single, definitive, math-on-the-web platform supporting the major browsers and operating systems.  It requires no setup on the part of the user (no plugins to download or software to install), so the page author can write web documents that include mathematics and be confident that users will be able to view it naturally and easily.  Simply include MathJax and some mathematics in a web page, and MathJax does the rest.</p> <p>Some of the main features of MathJax include:</p> <ul> <li> <p>High-quality display of LaTeX, MathML, and AsciiMath notation in HTML pages</p> </li> <li> <p>Supported in most browsers with no plug-ins, extra fonts, or special   setup for the reader</p> </li> <li> <p>Easy for authors, flexible for publishers, extensible for developers</p> </li> <li> <p>Supports math accessibility, cut-and-paste interoperability, and other   advanced functionality</p> </li> <li> <p>Powerful API for integration with other web applications</p> </li> </ul> <p>See http://www.mathjax.org/ for additional details about MathJax, and https://docs.mathjax.org for the MathJax documentation.</p>"},{"location":"static/node_modules/mathjax/#mathjax-components","title":"MathJax Components","text":"<p>MathJax version 3 uses files called components that contain the various MathJax modules that you can include in your web pages or access on a server through NodeJS.  Some components combine all the pieces you need to run MathJax with one or more input formats and a particular output format, while other components are pieces that can be loaded on demand when needed, or by a configuration that specifies the pieces you want to combine in a custom way.  For usage instructions, see the MathJax documentation.</p> <p>Components provide a convenient packaging of MathJax's modules, but it is possible for you to form your own custom components, or to use MathJax's modules directly in a node application on a server.  There are web examples showing how to use MathJax in web pages and how to build your own components, and node examples illustrating how to use components in node applications or call MathJax modules directly.</p>"},{"location":"static/node_modules/mathjax/#whats-in-this-repository","title":"What's in this Repository","text":"<p>This repository contains only the component files for MathJax, not the source code for MathJax (which are available in a separate MathJax source repository).  These component files are the ones served by the CDNs that offer MathJax to the web.  In version 2, the files used on the web were also the source files for MathJax, but in version 3, the source files are no longer on the CDN, as they are not what are run in the browser.</p> <p>The components are stored in the <code>es5</code> directory, and are in ES5 format for the widest possible compatibility.  In the future, we may make an <code>es6</code> directory containing ES6 versions of the components.</p>"},{"location":"static/node_modules/mathjax/#installation-and-use","title":"Installation and Use","text":""},{"location":"static/node_modules/mathjax/#using-mathjax-components-from-a-cdn-on-the-web","title":"Using MathJax components from a CDN on the web","text":"<p>If you are loading MathJax from a CDN into a web page, there is no need to install anything.  Simply use a <code>script</code> tag that loads MathJax from the CDN.  E.g.,</p> <pre><code>&lt;script id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"&gt;&lt;/script&gt;\n</code></pre> <p>See the MathJax documentation, the MathJax Web Demos, and the MathJax Component Repository for more information.</p>"},{"location":"static/node_modules/mathjax/#hosting-your-own-copy-of-the-mathjax-components","title":"Hosting your own copy of the MathJax Components","text":"<p>If you want to host MathJax from your own server, you can do so by installing the <code>mathjax</code> package using <code>npm</code> and moving the <code>es5</code> directory to an appropriate location on your server:</p> <pre><code>npm install mathjax@3\nmv node_modules/mathjax/es5 &lt;path-to-server-location&gt;/mathjax\n</code></pre> <p>Note that we are still making updates to version 2, so include <code>@3</code> when you install, since the latest chronological version may not be version 3.</p> <p>Alternatively, you can get the files via GitHub:</p> <pre><code>git clone https://github.com/mathjax/MathJax.git mj-tmp\nmv mj-tmp/es5 &lt;path-to-server-location&gt;/mathjax\nrm -rf mj-tmp\n</code></pre> <p>Then (in either case) you can use a script tag like the following:</p> <pre><code>&lt;script id=\"MathJax-script\" async src=\"&lt;url-to-your-site&gt;/mathjax/tex-chtml.js\"&gt;&lt;/script&gt;\n</code></pre> <p>where <code>&lt;url-to-your-site&gt;</code> is replaced by the URL to the location where you moved the MathJax files above.</p> <p>See the documentation for details.</p>"},{"location":"static/node_modules/mathjax/#using-mathjax-components-in-a-node-application","title":"Using MathJax components in a node application","text":"<p>To use MathJax components in a node application, install the <code>mathjax</code> package:</p> <pre><code>npm install mathjax@3\n</code></pre> <p>(we are still making updates to version 2, so you should include <code>@3</code> since the latest chronological version may not be version 3).</p> <p>Then require <code>mathjax</code> within your application:</p> <pre><code>require('mathjax').init({ ... }).then((MathJax) =&gt; { ... });\n</code></pre> <p>where the first <code>{ ... }</code> is a MathJax configuration, and the second <code>{ ... }</code> is the code to run after MathJax has been loaded.  E.g.</p> <pre><code>require('mathjax').init({\n  loader: {load: ['input/tex', 'output/svg']}\n}).then((MathJax) =&gt; {\n  const svg = MathJax.tex2svg('\\\\frac{1}{x^2-1}', {display: true});\n  console.log(MathJax.startup.adaptor.outerHTML(svg));\n}).catch((err) =&gt; console.log(err.message));\n</code></pre> <p>Note: this technique is for node-based application only, not for browser applications.  This method sets up an alternative DOM implementation, which you don't need in the browser, and tells MathJax to use node's <code>require()</code> command to load external modules.  This setup will not work properly in the browser, even if you webpack it or bundle it in other ways.</p> <p>See the documentation and the MathJax Node Repository for more details.</p>"},{"location":"static/node_modules/mathjax/#reducing-the-size-of-the-components-directory","title":"Reducing the Size of the Components Directory","text":"<p>Since the <code>es5</code> directory contains all the component files, so if you are only planning one use one configuration, you can reduce the size of the MathJax directory by removing unused components. For example, if you are using the <code>tex-chtml.js</code> component, then you can remove the <code>tex-mml-chtml.js</code>, <code>tex-svg.js</code>, <code>tex-mml-svg.js</code>, <code>tex-chtml-full.js</code>, and <code>tex-svg-full.js</code> configurations, which will save considerable space.  Indeed, you should be able to remove everything other than <code>tex-chtml.js</code>, and the <code>input/tex/extensions</code>, <code>output/chtml/fonts/woff-v2</code>, <code>adaptors</code>, <code>a11y</code>, and <code>sre</code> directories.  If you are using the results only on the web, you can remove <code>adaptors</code> as well.</p> <p>If you are not using A11Y support (e.g., speech generation, or semantic enrichment), then you can remove <code>a11y</code> and <code>sre</code> as well (though in this case you may need to disable the assistive tools in the MathJax contextual menu in order to avoid MathJax trying to load them when they aren't there).</p> <p>If you are using SVG rather than CommonHTML output (e.g., <code>tex-svg.js</code> rather than <code>tex-chtml.js</code>), you can remove the <code>output/chtml/fonts/woff-v2</code> directory.  If you are using MathML input rather than TeX (e.g., <code>mml-chtml.js</code> rather than <code>tex-chtml.js</code>), then you can remove <code>input/tex/extensions</code> as well.</p>"},{"location":"static/node_modules/mathjax/#the-component-files-and-pull-requests","title":"The Component Files and Pull Requests","text":"<p>The <code>es5</code> directory is generated automatically from the contents of the MathJax source repository.  You can rebuild the components using the command</p> <pre><code>npm run make-es5 --silent\n</code></pre> <p>Note that since the contents of this repository are generated automatically, you should not submit pull requests that modify the contents of the <code>es5</code> directory.  If you wish to submit a modification to MathJax, you should make a pull request in the MathJax source repository.</p>"},{"location":"static/node_modules/mathjax/#mathjax-community","title":"MathJax Community","text":"<p>The main MathJax website is http://www.mathjax.org, and it includes announcements and other important information.  A MathJax user forum for asking questions and getting assistance is hosted at Google, and the MathJax bug tracker is hosted at GitHub.</p> <p>Before reporting a bug, please check that it has not already been reported.  Also, please use the bug tracker (rather than the help forum) for reporting bugs, and use the user's forum (rather than the bug tracker) for questions about how to use MathJax.</p>"},{"location":"static/node_modules/mathjax/#mathjax-resources","title":"MathJax Resources","text":"<ul> <li>MathJax Documentation</li> <li>MathJax Components</li> <li>MathJax Source Code</li> <li>MathJax Web Examples</li> <li>MathJax Node Examples</li> <li>MathJax Bug Tracker</li> <li>MathJax Users' Group</li> </ul>"}]}